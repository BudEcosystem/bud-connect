{
  "model_info": {
    "description": "Whisper-1 is a general-purpose speech recognition model developed by OpenAI, based on the large-v2 version of the open-source Whisper model. It supports multilingual transcription and translation, automatic language detection, and robust performance with 50% fewer errors than specialized models. The model uses an encoder-decoder Transformer architecture and is trained on 680,000 hours of multilingual data, approaching human-level robustness on English speech.",
    "tags": [],
    "tasks": [],
    "use_cases": [
      "Podcast and video transcription",
      "Meeting and interview transcription",
      "Multilingual content accessibility",
      "Customer service call analysis",
      "Educational content transcription",
      "Legal and medical transcription",
      "Media monitoring and analysis",
      "Language learning applications",
      "Subtitle generation"
    ],
    "strengths": [
      "Supports transcription and translation in over 100 languages with automatic language detection",
      "Trained on 680,000 hours of multilingual data, including ~1/3 non-English content",
      "50% fewer errors than specialized models across diverse datasets",
      "Optimized API stack for faster processing and automatic punctuation/capitalization",
      "Handles audio formats like m4a, mp3, wav, and webm with 30-second chunk processing"
    ],
    "limitations": [
      "25 MB file size limit per request",
      "No real-time streaming support (requires separate Realtime API)",
      "Struggles with heavy accents, background noise, or non-standard speech patterns",
      "Translation output is limited to English only",
      "Processing time increases linearly with audio length"
    ],
    "languages": []
  },
  "model_evals": [],
  "model_name": "whisper-1",
  "provider": "openai",
  "extraction_date": "2025-07-22T11:54:00.289588"
}